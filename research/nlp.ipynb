{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron\n",
    "\n",
    "Using perceptron to perform binary classification of sentiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/envs/datastrategy2/lib/python3.12/site-packages (1.5.2)\n",
      "Collecting nltk\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /opt/anaconda3/envs/datastrategy2/lib/python3.12/site-packages (from scikit-learn) (2.1.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/anaconda3/envs/datastrategy2/lib/python3.12/site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/envs/datastrategy2/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/envs/datastrategy2/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.11.6-cp312-cp312-macosx_10_13_x86_64.whl.metadata (40 kB)\n",
      "Collecting tqdm (from nltk)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Downloading regex-2024.11.6-cp312-cp312-macosx_10_13_x86_64.whl (288 kB)\n",
      "Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, regex, click, nltk\n",
      "Successfully installed click-8.1.8 nltk-3.9.1 regex-2024.11.6 tqdm-4.67.1\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting torch\n",
      "  Downloading torch-2.2.2-cp312-none-macosx_10_9_x86_64.whl.metadata (25 kB)\n",
      "Collecting filelock (from transformers)\n",
      "  Downloading filelock-3.17.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.26.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.29.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/envs/datastrategy2/lib/python3.12/site-packages (from transformers) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/datastrategy2/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Using cached PyYAML-6.0.2-cp312-cp312-macosx_10_9_x86_64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/envs/datastrategy2/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Collecting requests (from transformers)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-macosx_10_12_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-macosx_10_12_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/envs/datastrategy2/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/envs/datastrategy2/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Collecting sympy (from torch)\n",
      "  Downloading sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Downloading jinja2-3.1.5-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting fsspec (from torch)\n",
      "  Downloading fsspec-2025.2.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Downloading MarkupSafe-3.0.2-cp312-cp312-macosx_10_13_universal2.whl.metadata (4.0 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->transformers)\n",
      "  Downloading charset_normalizer-3.4.1-cp312-cp312-macosx_10_13_universal2.whl.metadata (35 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->transformers)\n",
      "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->transformers)\n",
      "  Downloading urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->transformers)\n",
      "  Downloading certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Downloading transformers-4.49.0-py3-none-any.whl (10.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.2.2-cp312-none-macosx_10_9_x86_64.whl (150.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.8/150.8 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.29.1-py3-none-any.whl (468 kB)\n",
      "Downloading fsspec-2025.2.0-py3-none-any.whl (184 kB)\n",
      "Using cached PyYAML-6.0.2-cp312-cp312-macosx_10_9_x86_64.whl (183 kB)\n",
      "Downloading safetensors-0.5.3-cp38-abi3-macosx_10_12_x86_64.whl (436 kB)\n",
      "Downloading tokenizers-0.21.0-cp39-abi3-macosx_10_12_x86_64.whl (2.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.17.0-py3-none-any.whl (16 kB)\n",
      "Downloading jinja2-3.1.5-py3-none-any.whl (134 kB)\n",
      "Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "Downloading charset_normalizer-3.4.1-cp312-cp312-macosx_10_13_universal2.whl (196 kB)\n",
      "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "Downloading MarkupSafe-3.0.2-cp312-cp312-macosx_10_13_universal2.whl (14 kB)\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "Installing collected packages: mpmath, urllib3, sympy, safetensors, pyyaml, networkx, MarkupSafe, idna, fsspec, filelock, charset-normalizer, certifi, requests, jinja2, torch, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed MarkupSafe-3.0.2 certifi-2025.1.31 charset-normalizer-3.4.1 filelock-3.17.0 fsspec-2025.2.0 huggingface-hub-0.29.1 idna-3.10 jinja2-3.1.5 mpmath-1.3.0 networkx-3.4.2 pyyaml-6.0.2 requests-2.32.3 safetensors-0.5.3 sympy-1.13.3 tokenizers-0.21.0 torch-2.2.2 transformers-4.49.0 urllib3-2.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn nltk\n",
    "!pip install transformers torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.67\n",
      "Review: The food was exceptional, and the staff was super friendly!\n",
      "Sentiment: Negative\n",
      "\n",
      "Review: Terrible experience. Food was late and tasted awful.\n",
      "Sentiment: Negative\n",
      "\n",
      "Review: Amazing atmosphere but the drinks were overpriced.\n",
      "Sentiment: Negative\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Expanded sample labeled reviews\n",
    "reviews = [\n",
    "    \"The pasta was cooked perfectly, but the service was extremely slow.\",\n",
    "    \"Absolutely loved the sushi! Fresh and delicious. Will visit again.\",\n",
    "    \"The burger was overpriced and tasted bland. Not worth it.\",\n",
    "    \"Fantastic ambiance with great music, but the food was mediocre.\",\n",
    "    \"The waiter was rude and inattentive, but the steak was amazing.\",\n",
    "    \"Horrible experience! Cold food, bad service, never coming back.\",\n",
    "    \"Best pizza in town! Crispy crust and the perfect amount of cheese.\",\n",
    "    \"The dessert was heavenly, but the portions were too small.\",\n",
    "    \"Waited 40 minutes for food, and when it arrived, it was cold.\",\n",
    "    \"Great value for money! Generous portions and excellent quality.\"\n",
    "]\n",
    "\n",
    "# Corresponding labels: 1 = Positive, 0 = Negative\n",
    "labels = [0, 1, 0, 0, 0, 0, 1, 0, 0, 1]\n",
    "    \n",
    "# Convert text to numerical vectors\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(reviews).toarray()\n",
    "y = labels\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train Perceptron\n",
    "model = Perceptron()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate model performance\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Predict sentiment for new reviews\n",
    "new_reviews = [\n",
    "    \"The food was exceptional, and the staff was super friendly!\",\n",
    "    \"Terrible experience. Food was late and tasted awful.\",\n",
    "    \"Amazing atmosphere but the drinks were overpriced.\"\n",
    "]\n",
    "\n",
    "new_X = vectorizer.transform(new_reviews).toarray()\n",
    "predictions = model.predict(new_X)\n",
    "\n",
    "# Print results\n",
    "for review, prediction in zip(new_reviews, predictions):\n",
    "    sentiment = \"Positive\" if prediction == 1 else \"Negative\"\n",
    "    print(f\"Review: {review}\\nSentiment: {sentiment}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceptron : Takes a lot of time to compute and very inaccurate since the test set was very small. \n",
    "\n",
    "## VADER \n",
    "Using vader leads to more accuracy due to pre-built sentiment classification: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msentiment\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentimentIntensityAnalyzer\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Download the required package\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "\n",
    "# Download the required package\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Initialize Sentiment Analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Reviews to analyze\n",
    "reviews = [\n",
    "    \"Carve Cafe food is very bad in taste and greasy.\",\n",
    "    \"Terrible experience. Food was late and tasted awful.\",\n",
    "    \"Morrison serves the best chicken sandwich in town!\",\n",
    "    \"The food was exceptional, and the staff was super friendly!\",\n",
    "    \"The pasta was cooked perfectly, but the service was extremely slow.\", # Note the program didn't detect any negative sentiment in this review\n",
    "    \"The burger was greasy and the fries were soggy.\", # Same here, no negative sentiment detected\n",
    "] \n",
    "\n",
    "# Analyze Sentiment\n",
    "for review in reviews:\n",
    "    sentiment_score = sia.polarity_scores(review)\n",
    "    sentiment = \"Positive\" if sentiment_score['compound'] > 0 else \"Negative\"\n",
    "    print(f\"Review: {review}\\nSentiment: {sentiment} (Score: {sentiment_score})\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-MEANS Clustering and TF-IDF\n",
    "\n",
    "TF-IDF Vectorization:\n",
    "Reviews are transformed into numerical features that capture the importance of words and phrases (including bigrams) while ignoring common stop words.\n",
    "\n",
    "K-Means Clustering:\n",
    "The reviews are grouped into clusters. You can adjust num_clusters based on how many distinct topics/products you expect (for example, clusters might naturally emerge for \"chicken\", \"greasy food\", \"pasta\", etc.).\n",
    "\n",
    "Top Terms Extraction:\n",
    "For each cluster, we sort the cluster center’s feature weights and print the top terms. These terms give insight into what the cluster is mainly discussing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Cluster 0 ---\n",
      "The burger was greasy and the fries were soggy.\n",
      "The pizza was too greasy, but the crust was perfect.\n",
      "Greasy food isn't my thing, especially when it's overcooked.\n",
      "\n",
      "Top terms in this cluster: ['greasy', 'crust', 'soggy', 'greasy crust', 'greasy fries', 'crust perfect', 'perfect', 'fries', 'pizza', 'pizza greasy']\n",
      "\n",
      "\n",
      "--- Cluster 1 ---\n",
      "Amazing grilled chicken salad with fresh veggies.\n",
      "The fried chicken is superb, perfectly seasoned.\n",
      "Loved the chicken wrap; it was light and full of flavor.\n",
      "The deep-fried chicken was overly greasy and unappetizing.\n",
      "\n",
      "Top terms in this cluster: ['chicken', 'fried chicken', 'fried', 'superb perfectly', 'seasoned', 'perfectly seasoned', 'superb', 'perfectly', 'chicken superb', 'chicken wrap']\n",
      "\n",
      "\n",
      "--- Cluster 2 ---\n",
      "I love the crispy chicken wings and the spicy sauce.\n",
      "Chicken tenders here are delicious and crispy.\n",
      "I found the pasta a bit bland and the sauce too oily.\n",
      "\n",
      "Top terms in this cluster: ['crispy', 'sauce', 'tenders delicious', 'chicken tenders', 'tenders', 'delicious crispy', 'delicious', 'chicken', 'bit', 'bit bland']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Sample restaurant reviews focusing on product mentions\n",
    "reviews = [\n",
    "    \"I love the crispy chicken wings and the spicy sauce.\",\n",
    "    \"The burger was greasy and the fries were soggy.\",\n",
    "    \"Amazing grilled chicken salad with fresh veggies.\",\n",
    "    \"The pizza was too greasy, but the crust was perfect.\",\n",
    "    \"Chicken tenders here are delicious and crispy.\",\n",
    "    \"I found the pasta a bit bland and the sauce too oily.\",\n",
    "    \"The fried chicken is superb, perfectly seasoned.\",\n",
    "    \"Greasy food isn't my thing, especially when it's overcooked.\",\n",
    "    \"Loved the chicken wrap; it was light and full of flavor.\",\n",
    "    \"The deep-fried chicken was overly greasy and unappetizing.\"\n",
    "]\n",
    "\n",
    "# Step 1: Convert text to TF-IDF features\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\", ngram_range=(1,2))\n",
    "X = vectorizer.fit_transform(reviews)\n",
    "\n",
    "# Step 2: Apply K-Means clustering\n",
    "num_clusters = 3  # Adjust as needed for your dataset\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "clusters = kmeans.fit_predict(X)\n",
    "\n",
    "# Step 3: Output the reviews grouped by cluster and list top terms per cluster\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "for i in range(num_clusters):\n",
    "    print(f\"--- Cluster {i} ---\")\n",
    "    cluster_reviews = [reviews[j] for j in range(len(reviews)) if clusters[j] == i]\n",
    "    for review in cluster_reviews:\n",
    "        print(review)\n",
    "    \n",
    "    # Identify top terms in the cluster\n",
    "    # Note: We use the cluster centers to rank terms. They are in the TF-IDF space.\n",
    "    order_centroids = kmeans.cluster_centers_[i].argsort()[::-1]\n",
    "    top_terms = [terms[ind] for ind in order_centroids[:10]]\n",
    "    print(\"\\nTop terms in this cluster:\", top_terms)\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datastrategy2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
